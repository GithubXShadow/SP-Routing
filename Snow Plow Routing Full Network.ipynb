{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pulp\n",
    "import random\n",
    "import math\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_network():\n",
    "    '''\n",
    "        input: network.dat\n",
    "        output:\n",
    "            num_zones: number of zones within the network\n",
    "            num_nodes: number of nodes in the network\n",
    "            num_links: number of links in the network\n",
    "            node_detail: node id and zone which the node belong to\n",
    "            node_id: match the node id to 1~num_nodes to save memory\n",
    "            link_detail:The information associated with a specific link\n",
    "                link_detail[0] upstream node\n",
    "                link_detail[1] downstream node\n",
    "                link_detail[2] Num of left turn bay\n",
    "                link_detail[3] Num of right turn bay\n",
    "                link_detail[4] length of link\n",
    "                link_detail[5] Num of lanes\n",
    "                link_detail[6] Traffic flow mode\n",
    "                link_detail[7] posted speed limit adjustment margin\n",
    "                link_detail[8] posted speed limit\n",
    "                link_detail[9] maximum service flow rate for the first link\n",
    "            link_id: a matrix to find a specific link based on origin node nd destination node \n",
    "   '''\n",
    "    global num_zones,num_nodes,num_links,node_detail,node_id,link_detail,link_id\n",
    "    file=open('network.dat','r')\n",
    "    i=0\n",
    "    network_basic=list()\n",
    "    node_list=list()\n",
    "    node_id={}\n",
    "    link_detail_list=list()\n",
    "    for line in file:\n",
    "        line_list_temp=line.split()\n",
    "        # Read the first line of network.dat\n",
    "        if i==0:\n",
    "            num_zones=int(line_list_temp[0])\n",
    "            num_nodes=int(line_list_temp[1])\n",
    "            num_links=int(line_list_temp[2])\n",
    "            num_shortest_path=int(line_list_temp[3])\n",
    "            zone_flag=int(line_list_temp[4])\n",
    "            link_id=np.zeros((num_nodes,num_nodes))\n",
    "        #Read the node information from network.dat\n",
    "        elif len(line_list_temp)<3:\n",
    "            node_list.append([int(line_list_temp[0]),int(line_list_temp[1])])\n",
    "            node_id.update({int(line_list_temp[0]):i-1})\n",
    "        #Read the link information from network.dat\n",
    "        else:\n",
    "#             for j in line_list_temp:\n",
    "#                 if '+' in j and len(j)>2:\n",
    "#                     check=j.split('+')\n",
    "#                     line_list_temp.pop(6) \n",
    "#                     line_list_temp.insert(6,check[0])\n",
    "#                     line_list_temp.insert(7,check[1])\n",
    "            line_list_temp=[float(j) for j in line_list_temp]\n",
    "            link_detail_list.append(line_list_temp)\n",
    "            link_id[node_id[line_list_temp[0]],node_id[line_list_temp[1]]]=i-num_nodes-1\n",
    "        i=i+1\n",
    "        \n",
    "    node_detail=np.matrix(node_list)\n",
    "    link_detail=np.matrix(link_detail_list)\n",
    "    \n",
    "    return  num_zones,num_nodes,num_links,node_detail,node_id,link_detail,link_id\n",
    "def read_flow():\n",
    "    '''\n",
    "        input: outflow.dat\n",
    "        output: \n",
    "            link_volume - matrix\n",
    "    '''\n",
    "    global link_volume\n",
    "    volume_time=[]\n",
    "    file=open('OutFlow.dat')\n",
    "    i=1\n",
    "    link_volume=[]\n",
    "    link_volume_one_time_interval=[]\n",
    "    for line in file:\n",
    "        line_list=line.split()\n",
    "        if i>6 : # skip the first 6 lines\n",
    "            if len(line_list)==1: #find the line indicating time interval\n",
    "                volume_time.append(float(line_list[0]))\n",
    "                link_volume.append(link_volume_one_time_interval)\n",
    "                link_volume_one_time_interval=[]\n",
    "            else:\n",
    "                line_list=[float(j) for j in line_list]\n",
    "                link_volume_one_time_interval.extend(line_list)\n",
    "        i=i+1\n",
    "    link_volume.append(link_volume_one_time_interval)\n",
    "    link_volume.pop(0)\n",
    "    link_volume=np.array(link_volume)\n",
    "    return link_volume\n",
    "def read_speed():\n",
    "    '''\n",
    "        input:OutLinkSpeedAll.dat\n",
    "        output: \n",
    "            link_speed -matrix\n",
    "    '''\n",
    "    global link_speed\n",
    "    speed_time=[]\n",
    "    file=open('OutLinkSpeedAll.dat')\n",
    "    i=1\n",
    "    link_speed=[]\n",
    "    link_speed_one_time_interval=[]\n",
    "    for line in file:\n",
    "        line_list=line.split()\n",
    "        if i>6 : # skip the first 6 lines\n",
    "            if len(line_list)==1: #find the line indicating time interval\n",
    "                speed_time.append(float(line_list[0]))\n",
    "                link_speed.append(link_speed_one_time_interval)\n",
    "                link_speed_one_time_interval=[]\n",
    "            else:\n",
    "                line_list=[float(j) for j in line_list]\n",
    "                link_speed_one_time_interval.extend(line_list)\n",
    "        i=i+1\n",
    "    link_speed.append(link_speed_one_time_interval)\n",
    "    link_speed.pop(0)\n",
    "    link_speed=np.array(link_speed)\n",
    "    return link_speed\n",
    "\n",
    "\n",
    "def read_xy():\n",
    "    '''\n",
    "        input: xy.dat\n",
    "        output: nodexy: a dictionary storing the latitude and longtitude of \n",
    "            each node\n",
    "    '''\n",
    "    global nodexy\n",
    "    nodexy={}\n",
    "    file=open('xy.dat')\n",
    "    for line in file:\n",
    "        line_list_temp=line.split()\n",
    "        line_list_temp=[float(j) for j in line_list_temp]\n",
    "        nodexy.update({line_list_temp[0]:[line_list_temp[1]/1000000.0*51.33,-line_list_temp[2]/1000000.0*68]})\n",
    "    return\n",
    "\n",
    "def read_snow():\n",
    "    '''\n",
    "        ipnut: weather.dat\n",
    "        output: snow_detail. \n",
    "                    snow_detail[i,0] snow intensity at time interval i\n",
    "                    snow_detail[i,1] start time of time interval i\n",
    "                    snow_detail[i,2] end time of time interval i\n",
    "    '''\n",
    "    global snow_detail,snow_interval\n",
    "    file=open('weather.dat')\n",
    "    snow_detail=[]\n",
    "    i=0\n",
    "    for line in file: \n",
    "        if i==0:\n",
    "            num_interval=float(line.split()[0])\n",
    "            #print(num_interval,i,line)\n",
    "            snow_interval=planning_horizon/num_interval\n",
    "        elif i!= num_interval+1:\n",
    "            line_list_temp= line.split()\n",
    "            snow_detail.append([float(line_list_temp[2]),float(line_list_temp[3]),float(line_list_temp[4])])\n",
    "        i=i+1\n",
    "    return\n",
    "def read_snowaccum(): \n",
    "    global sa_factors\n",
    "    sa_factors=[]\n",
    "    file=open('SnowAccuFactor.dat')\n",
    "    for line in file: \n",
    "        if len(line)>4:\n",
    "            line_list_temp=[j for j in line.split()]\n",
    "            sa_factors.append([float(j) for j in line_list_temp])\n",
    "    return\n",
    "# def read_scenario()\n",
    "#     '''\n",
    "#         Input: Scenario.dat\n",
    "#         output:\n",
    "#     '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def link_class_partrition():\n",
    "    '''\n",
    "        input: \n",
    "            link_volume\n",
    "        output:\n",
    "            link_class\n",
    "    '''\n",
    "    global link_class\n",
    "    #Calculate the threshold of different service level\n",
    "    service_threshold=[]\n",
    "    service_threshold.append(np.percentile(np.amax(link_volume,axis=0),80))\n",
    "    service_threshold.append(np.percentile(np.amax(link_volume,axis=0),40))\n",
    "    service_threshold.append(np.percentile(np.amax(link_volume,axis=0),0))\n",
    "    i=0\n",
    "    link_class=[]\n",
    "    for link in link_detail:\n",
    "        if max(link_volume[:,i])>=service_threshold[0] or (link[11]!=5) :\n",
    "            link_class.append(1)\n",
    "        elif max(link_volume[:,i])>=service_threshold[1]: #or if it is a bus routes, need to modify later\n",
    "            link_class.append(2)\n",
    "        else: \n",
    "            link_class.append(3)\n",
    "        i=i+1\n",
    "    return\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def distance_between_link(link_ID1,link_ID2,link_xy):\n",
    "#     xyoflink1=[]\n",
    "#     xyoflink1.extend([(nodexy[link_detail[link_ID1,0]][0]+nodexy[link_detail[link_ID1,1]][0])/2])\n",
    "#     xyoflink1.extend([(nodexy[link_detail[link_ID1,0]][1]+nodexy[link_detail[link_ID1,1]][1])/2])\n",
    "#     xyoflink2=[]\n",
    "#     xyoflink2.extend([(nodexy[link_detail[link_ID2,0]][0]+nodexy[link_detail[link_ID2,1]][0])/2])\n",
    "#     xyoflink2.extend([(nodexy[link_detail[link_ID2,0]][1]+nodexy[link_detail[link_ID2,1]][1])/2])\n",
    "#     distance=math.sqrt((xyoflink1[0]-xyoflink2[0])**2+(xyoflink1[0]-xyoflink2[0])**2)\n",
    "    distance=math.sqrt((link_xy[link_ID1][0]-link_xy[link_ID2][0])**2+(link_xy[link_ID1][1]-link_xy[link_ID2][1])**2)\n",
    "    return distance\n",
    "def c_link_xy (links):\n",
    "    global link_xy\n",
    "    link_xy=[]\n",
    "    for link_ID1 in links: \n",
    "        link_xy.append([(nodexy[link_detail[link_ID1,0]][0]+nodexy[link_detail[link_ID1,1]][0])/2,\n",
    "                       (nodexy[link_detail[link_ID1,0]][1]+nodexy[link_detail[link_ID1,1]][1])/2])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def net_work_partrition(network_link_list,num_cluster,center,seed_links):\n",
    "    '''\n",
    "    input: \n",
    "        network_link_list: a list of link id that needs to be partitioned \n",
    "        num_cluster: the network would be partitioned into num_cluster sets\n",
    "        center: optional input, the location of the depot\n",
    "        seed_links: optional input, the seed link for each cluster\n",
    "    output\n",
    "        clusters: a list of link contained in each cluster\n",
    "    '''\n",
    "    #global d\n",
    "    if not center: \n",
    "        centertemp=random.choice(network_link_list)\n",
    "        center=link_detail[centertemp,0]\n",
    "    if not seed_links: # in this case, we do not have a seedlink,but have a center depot\n",
    "        for i in range(num_cluster):\n",
    "            maxdistance=0\n",
    "            linkcandidate=0\n",
    "            #Go over all the candidate links to find the one which locates furtherest to the depot and other seedlinks\n",
    "            for link in network_link_list:\n",
    "                #calculate distance form link candidate to the depot\n",
    "                distance_to_depot=math.sqrt((nodexy[link_detail[link,0]][0]-nodexy[center][0])**2\n",
    "                                            +(nodexy[link_detail[link,0]][1]-nodexy[center][1])**2)\n",
    "                total_distance=distance_to_depot\n",
    "                #calculate distance form link candidate to other seed links\n",
    "                for seed_link in seed_links: \n",
    "                    total_distance=total_distance*distance_between_link(seed_link,link,link_xy)\n",
    "                if total_distance>maxdistance and link not in seed_links:\n",
    "                    maxdistance=total_distance\n",
    "                    linkcandidate=link\n",
    "            seed_links.append(linkcandidate)\n",
    "    print('The following links are selected as seed link: \\n',seed_links)\n",
    "    \n",
    "    #After the seed_links are selected, solve the integer problem to identify the link clusters\n",
    "    cluster_link_index=[i for i in range(len(network_link_list))]\n",
    "    cluster_zone_index=[i for i in range(num_cluster)]\n",
    "    link_length=[link_detail[i,4] for i in network_link_list]\n",
    "    \n",
    "    L=float(sum(link_length)/num_cluster*0.95)\n",
    "    U=float(sum(link_length)/num_cluster*1.05)\n",
    "\n",
    "    prob=pulp.LpProblem('Link Cluster Problem',pulp.LpMinimize)\n",
    "    link_cluster=pulp.LpVariable.dicts(\"Cluster\",(cluster_link_index,cluster_zone_index),0,1,pulp.LpInteger)\n",
    "    d=np.zeros(shape=(len(network_link_list),num_cluster))\n",
    "    for l in cluster_link_index:\n",
    "        for z in cluster_zone_index:\n",
    "            #print(l,z,network_link_list[l],seed_links[z])\n",
    "            d[l,z]=distance_between_link(network_link_list[l],seed_links[z],link_xy)\n",
    "            \n",
    "    prob += pulp.lpSum([d[l,z]*link_cluster[l][z]] for l in cluster_link_index for z in cluster_zone_index), \"Total distance from link to assigned seed link\"\n",
    "    for l in cluster_link_index:\n",
    "        prob += pulp.lpSum([link_cluster[l][z] for z in cluster_zone_index]) == 1\n",
    "    for z in cluster_zone_index:\n",
    "        prob += pulp.lpSum([float(link_length[l])*link_cluster[l][z]] for l in cluster_link_index )>=L\n",
    "        prob += pulp.lpSum([float(link_length[l])*link_cluster[l][z]] for l in cluster_link_index)<=U\n",
    "        \n",
    "    prob.writeLP(\"Link Cluster Problem.lp\")\n",
    "    # The problem is solved using PuLP's choice of Solver\n",
    "    prob.solve()\n",
    "    print(\"Solution Status:\", pulp.LpStatus[prob.status])\n",
    "    network_link_cluster_list=[[] for i in range(num_cluster)]\n",
    "    for l in cluster_link_index:\n",
    "        for z in cluster_zone_index:\n",
    "            if pulp.value(link_cluster[l][z])==1:\n",
    "                network_link_cluster_list[z].append(network_link_list[l])\n",
    "    \n",
    "    return network_link_cluster_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def network_partrition_nearest_heuristic(network_link_list,num_cluster,center,seed_links):\n",
    "    '''\n",
    "    input: \n",
    "        network_link_list: a list of link id that needs to be partitioned \n",
    "        num_cluster: the network would be partitioned into num_cluster sets\n",
    "        center: optional input, the location of the depot\n",
    "        seed_links: optional input, the seed link for each cluster\n",
    "    output\n",
    "        clusters: a list of link contained in each cluster\n",
    "    '''\n",
    "#     global d\n",
    "    if not center: \n",
    "        centertemp=random.choice(network_link_list)\n",
    "        center=link_detail[centertemp,0]\n",
    "    if not seed_links: # in this case, we do not have a seedlink,but have a center depot\n",
    "        for i in range(num_cluster):\n",
    "            maxdistance=0\n",
    "            linkcandidate=0\n",
    "            #Go over all the candidate links to find the one which locates furtherest to the depot and other seedlinks\n",
    "            for link in network_link_list:\n",
    "                #calculate distance form link candidate to the depot\n",
    "#                 Distancecalculated based on the middle points of the link\n",
    "#                 distance_to_depot=math.sqrt(((nodexy[link_detail[link,0]][0]+nodexy[link_detail[link,1]][0])/2-nodexy[center][0])**2+\n",
    "#                                             ((nodexy[link_detail[link,0]][1]+nodexy[link_detail[link,1]][1])/2-nodexy[center][1])**2) \n",
    "                #Distance calculated based on the up stream node\n",
    "                distance_to_depot=math.sqrt((nodexy[link_detail[link,0]][0]-nodexy[center][0])**2+\n",
    "                                             (nodexy[link_detail[link,0]][1]-nodexy[center][1])**2) \n",
    "                total_distance=distance_to_depot\n",
    "                #calculate distance form link candidate to other seed links\n",
    "                for seed_link in seed_links: \n",
    "                    total_distance=total_distance*distance_between_link(seed_link,link,link_xy)\n",
    "                if total_distance>maxdistance and link not in seed_links:\n",
    "                    maxdistance=total_distance\n",
    "                    linkcandidate=link\n",
    "            seed_links.append(linkcandidate)\n",
    "    print('The following links are selected as seed link: \\n',seed_links)\n",
    "    \n",
    "    #After the seed_links are selected, solve the integer problem to identify the link clusters\n",
    "    cluster_link_index=[i for i in range(len(network_link_list))]\n",
    "    cluster_zone_index=[i for i in range(num_cluster)]\n",
    "    link_length=[link_detail[i,4] for i in network_link_list]\n",
    "    \n",
    "#     L=float(sum(link_length)/num_cluster*0.8)\n",
    "#     U=float(sum(link_length)/num_cluster*1.2)\n",
    "\n",
    "#     prob=pulp.LpProblem('Link Cluster Problem',pulp.LpMinimize)\n",
    "#     link_cluster=pulp.LpVariable.dicts(\"Cluster\",(cluster_link_index,cluster_zone_index),0,1,pulp.LpInteger)\n",
    "    print('Prepare distance matrix between links and seed links')\n",
    "    d=np.zeros(shape=(len(network_link_list),num_cluster))\n",
    "    for l in range(len(network_link_list)):\n",
    "        for z in range(num_cluster):\n",
    "            #print(l,z,network_link_list[l],seed_links[z])\n",
    "            d[l,z]=distance_between_link(network_link_list[l],seed_links[z],link_xy)\n",
    "    \n",
    "#     prob += pulp.lpSum([d[l,z]*link_cluster[l][z]] for l in cluster_link_index for z in cluster_zone_index), \n",
    "#\"Total distance from link to assigned seed link\"\n",
    "#     for l in cluster_link_index:\n",
    "#         prob += pulp.lpSum([link_cluster[l][z] for z in cluster_zone_index]) == 1\n",
    "#     for z in cluster_zone_index:\n",
    "#         prob += pulp.lpSum([float(link_length[l])*link_cluster[l][z]] for l in cluster_link_index )>=L\n",
    "#         prob += pulp.lpSum([float(link_length[l])*link_cluster[l][z]] for l in cluster_link_index)<=U\n",
    "        \n",
    "#     prob.writeLP(\"Link Cluster Problem.lp\")\n",
    "#     # The problem is solved using PuLP's choice of Solver\n",
    "#     prob.solve()\n",
    "#     print(\"Solution Status:\", pulp.LpStatus[prob.status])\n",
    "    network_link_cluster_list=[[] for i in range(num_cluster)]\n",
    "    for l in cluster_link_index:\n",
    "        s_distance=999999\n",
    "        c_seed=99\n",
    "        for z in cluster_zone_index:\n",
    "            if d[l,z]<s_distance:\n",
    "                s_distance=d[l,z]\n",
    "                c_seed=z\n",
    "        network_link_cluster_list[c_seed].append(network_link_list[l])\n",
    "        \n",
    "#             if pulp.value(link_cluster[l][z])==1:\n",
    "#                 network_link_cluster_list[z].append(network_link_list[l])\n",
    "            #Added for the nearest heuristic\n",
    "            \n",
    "    \n",
    "    return network_link_cluster_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def multi_commodity_model(cluster):\n",
    "#     '''\n",
    "#         input:\n",
    "#             clusters: link ID of links will be covered by the depot\n",
    "#             link: link length of all links within the clusters\n",
    "#             s_speed: service speed\n",
    "#             d_speed: deadhead speed\n",
    "#         intermediat output: \n",
    "#             x: 0 or 1 integer indicate whether an arc is serviced or not\n",
    "#             y: an integer indicates whether the edge is traversed by associated vehicle\n",
    "#         output: \n",
    "#             route plan\n",
    "#     '''\n",
    "#     #Reconstruct the network\n",
    "#     #Duplicate multiple lane \n",
    "#     new_cluster=[]\n",
    "#     ijindicator=[]\n",
    "#     for link in cluster: \n",
    "#         for lane in range(0,int(link_detail[link,5])):\n",
    "#             new_cluster.append(link)\n",
    "#             if link_detail[link,0]>link_detail[link,2]:\n",
    "#                 ijindicator.append(1)\n",
    "#             else:\n",
    "#                 ijindicator.append(0)\n",
    "#     #Add the depot and fictious class (needs to be done)\n",
    "    \n",
    "#     #Create the index for the vehicle class\n",
    "#     cluster_link_index=[i for i in range(len(new_cluster))]\n",
    "#     cluster_class_index=[i for i in range(3)]\n",
    "    \n",
    "#     #decision variables\n",
    "#     service_links=pulp.LpVariable.dicts(\"service_link\",(cluster_link_index,cluster_class_index),0,1,pulp.LpInteger)\n",
    "#     traversed_links=pulp.LpVariable.dicts(\"traversed_link\",(cluster_link_index,cluster_class_index),0,10,pulp.LpInteger)\n",
    "    \n",
    "#     prob=pulp.LpProblem('Snowplowrouting Problem',pulp.LpMinimize)\n",
    "    \n",
    "#     #input variables\n",
    "#     service_time=[link_detail[i,4]/1024/s_speed for i in new_cluster]\n",
    "#     deadhead_time=[link_detail[i,4]/1024/d_speed for i in new_cluster]\n",
    "#     #objective function\n",
    "#     prob += pulp.lpSum(service_links[l][c]*float(service_time[l])+traversed_links[l][c]*float(deadhead_time[l]) for l in cluster_link_index for c in cluster_class_index)\n",
    "    \n",
    "#     #constraints 2.2\n",
    "#     for p in cluster_class_index:\n",
    "#         if p==0:\n",
    "#             prob += pulp.lpSum(service_links[l][0]*float(service_time[l])+traversed_links[l][0]*float(deadhead_time[l]) for l in cluster_link_index)>=0.0\n",
    "#         else:\n",
    "#             prob += pulp.lpSum(service_links[l][p]*float(service_time[l])+traversed_links[l][p]*float(deadhead_time[l]) for l in cluster_link_index)-pulp.lpSum(service_links[l][p-1]*float(service_time[l])+traversed_links[l][p-1]*float(deadhead_time[l]) for l in cluster_link_index)>=0.01\n",
    "#     #Completion time 2.3\n",
    "#     for l in cluster_link_index:\n",
    "#         print(pulp.lpSum([service_links[l][z] for z in cluster_class_index]))\n",
    "#         prob += pulp.lpSum([service_links[l][z] for z in cluster_class_index]) == 1  #2.5\n",
    "#     #2.7\n",
    "#     prob += pulp.lpSum(service_links[l][z]-2*service_links[l][z]*ijindicator[l] for l in cluster_link_index for z in cluster_class_index)==0\n",
    "#     prob += pulp.lpSum(service_links)\n",
    "#     print(service_links[l][z]*ijindicator[l] for l in cluster_link_index) #for z in cluster_class_index)\n",
    "#     #2.8\n",
    "    \n",
    "    \n",
    "    \n",
    "#     prob.writeLP(\"Multi Commodity Routing.lp\")\n",
    "#     # The problem is solved using PuLP's choice of Solver\n",
    "#     prob.solve()\n",
    "#     print(\"Solution Status:\", pulp.LpStatus[prob.status])\n",
    "    \n",
    "# #     network_link_cluster_list=[[] for i in range(num_cluster)]\n",
    "# #     for l in cluster_link_index:\n",
    "# #         for z in cluster_zone_index:\n",
    "# #             if pulp.value(link_cluster[l][z])==1:\n",
    "# #                 network_link_cluster_list[z].append(network_link_list[l])\n",
    "    \n",
    "#     return\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initial settings and read inputs\n",
    "print('Read the network.dat')\n",
    "read_network()\n",
    "# Read the traffic volume outflow.dat\n",
    "# print('Read the OutFlow.dat')\n",
    "read_flow()\n",
    "read_speed()\n",
    "print('Read the xy.dat')\n",
    "read_xy()\n",
    "# Parameter Setting\n",
    "\n",
    "# The input\n",
    "global s_speed,d_speed,num_simulation_interval,simulation_length,planning_horizon\n",
    "s_speed= 0.5 #mile per minute\n",
    "d_speed= 0.5 #mile per minute\n",
    "Num_Zones=4\n",
    "Num_Snowplows=10\n",
    "num_link_classes=3\n",
    "depots=[14522]\n",
    "num_simulation_interval=1440/5\n",
    "simulation_length=5.0\n",
    "planning_horizon=1440.0\n",
    "VOT=5 # $/minute\n",
    "OpCost=0.25 # $/minute\n",
    "link_id=[i for i in range(num_links)]\n",
    "print('Read the snow information')\n",
    "read_snow()\n",
    "read_snowaccum()\n",
    "c_link_xy(link_id)\n",
    "#Intial Value for the service finish time of each lane \n",
    "service_finished_time=[]\n",
    "i=0\n",
    "for link in link_detail:\n",
    "    service_finished_time.append([])\n",
    "    service_finished_time[i].extend([planning_horizon]*link[0,5])\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Divided the link into different catogories\n",
    "# print('Calibrate Service Class')\n",
    "# link_class_partrition()\n",
    "\n",
    "#Determine the subzone covered by each depot\n",
    "print('Determing subzone covered by depot each depot')\n",
    "#sub_service_zone=net_work_partrition(subzone,len(depots),[],depots)\n",
    "sub_service_zone=net_work_partrition(link_id,Num_Zones,depots[0],[])\n",
    "\n",
    "#Construct sub_cluster within each subzone\n",
    "print('Determing subcluster covered by each snowplow')\n",
    "#sub_service_clusters=net_work_partrition(subzone,Num_Snowplows,depots[0],[])\n",
    "\n",
    "#sub_vehicle_clusters=network_partrition_nearest_heuristic(sub_service_zone[2],Num_Snowplows,[],[])\n",
    "# Locating M geographically dispersed arcs of A to serve as seed arcs for the M vehicles\n",
    "# Partrition the network into /Num_Snowplowss/ subarea. \n",
    "# for cluster in sub_service_clusters:\n",
    "#     #Solve the linear integer for each cluster\n",
    "#     check_temp=1\n",
    "    \n",
    "\n",
    "print('Assign links to specified vehicle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sub_service_cluster=net_work_partrition(sub_service_zone[2],Num_Snowplows,14103,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sub_service_cluster=network_partrition_nearest_heuristic(sub_service_zone[2],Num_Snowplows,14103,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_highlight(linklist):\n",
    "    #This function will highlight the linklist in a given network\n",
    "    plt.figure() \n",
    "    Gnormal=nx.Graph()\n",
    "    Gred=nx.Graph()\n",
    "    read_network()\n",
    "    read_xy()\n",
    "\n",
    "    for nodeinf in node_detail:\n",
    "        node=nodeinf[0,0]\n",
    "        Gnormal.add_node(node,pos=(nodexy[node][0],nodexy[node][1]))\n",
    "        Gred.add_node(node,pos=(nodexy[node][0],nodexy[node][1]))\n",
    "#         Gnormal.add_node(node,pos=(nodexy[node][1],nodexy[node][0]))\n",
    "#         Gred.add_node(node,pos=(nodexy[node][1],nodexy[node][0]))\n",
    "    counter=0\n",
    "    red_link=[]\n",
    "    normal_link=[]\n",
    "    for linkinf in link_detail: \n",
    "        if counter in linklist:\n",
    "            red_link.append((linkinf[0,0],linkinf[0,1]))\n",
    "            Gred.add_edge(linkinf[0,0],linkinf[0,1])        \n",
    "        else:\n",
    "            normal_link.append((linkinf[0,0],linkinf[0,1]))\n",
    "            Gnormal.add_edge(linkinf[0,0],linkinf[0,1])\n",
    "        counter=counter+1\n",
    "            \n",
    "    pos=nx.get_node_attributes(Gnormal,'pos')\n",
    "    nx.draw(Gnormal,pos,node_size=1)\n",
    "    nx.draw(Gred,pos,edge_color='r',width=4,node_size=4)\n",
    "    #nx.draw_networkx_nodes(G,pos,node_size=50)\n",
    "    plt.draw()\n",
    "   \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def snow_penalty(service_finished_time,snow_detail,sa_factors):\n",
    "    '''\n",
    "    input: \n",
    "        service_finished_time: the expected service finished time of each link\n",
    "        snow_precipitation: The snow precipitation rate \n",
    "        sa_factors: The speed/capacity reduction rate caused by different level of snow accumulation\n",
    "    Output:   \n",
    "        Tot_Delay: delay due to the snow accumulation\n",
    "    Intermediate variable: \n",
    "        snow_depth[]: snowdepth of each lane at current time interval\n",
    "    '''\n",
    "    average_snow_depth=[]\n",
    "    j=0\n",
    "    Tot_Delay=0\n",
    "    for link in link_detail: #Calculate the penalty on each link\n",
    "        #Calculate the reduced speed based on the snow depth\n",
    "        st =service_finished_time[j]\n",
    "        snow_depth_temp=0\n",
    "        snow_depth=[0]*math.floor(link[0,5])\n",
    "        for t in range (math.floor(num_simulation_interval)):\n",
    "            penaltytemp=0\n",
    "            ####Calculate the snow depth\n",
    "            for snow in snow_detail:\n",
    "                if snow[2] > (t+0.5)*simulation_length and snow[1] < (t+0.5)*simulation_length:\n",
    "                    current_snow=snow[0]\n",
    "            for i in range(math.floor(link[0,5])):\n",
    "                if service_finished_time[j][i] <(t+1)*simulation_length and service_finished_time[j][i] >t*simulation_length:\n",
    "                    snow_depth[i]=0\n",
    "                else:\n",
    "                    snow_depth[i]=snow_depth[i]+current_snow*snow_interval/60\n",
    "            average_snow_depth=sum(snow_depth)/link[0,5]\n",
    "            ####Finish calculating the snow depth\n",
    "            \n",
    "            ### Calculate the affected speed\n",
    "            origin_speed=link_speed[math.floor(t*simulation_length),j]\n",
    "            k=0\n",
    "            while average_snow_depth>sa_factors[k][0]:\n",
    "                k=k+1\n",
    "            speed_reduction=sa_factors[k][1]\n",
    "            capacity_reduction=sa_factors[k][2]\n",
    "            reduced_speed=origin_speed*speed_reduction #account for the speed reduction\n",
    "            #reduced_speed= #account for the capacity reduction use the BPR function\n",
    "            penaltytemp=link_volume[math.floor(t*simulation_length),j]*(origin_speed-reduced_speed)/origin_speed*simulation_length\n",
    "            Tot_Delay=Tot_Delay+penaltytemp\n",
    "        j=j+1\n",
    "    return Tot_Delay\n",
    "#def sp_cost(start_node,target_node)\n",
    "def link_snow_penalty (link,service_finished_time):\n",
    "    '''\n",
    "    This subroutine update the total snow_penalty when only service time of one link is updated\n",
    "\n",
    "    input: \n",
    "        service_finished_time: the expected service finished time of each link\n",
    "        snow_precipitation: The snow precipitation rate \n",
    "        sa_factors: The speed/capacity reduction rate caused by different level of snow accumulation\n",
    "    Output:   \n",
    "        Tot_Delay: delay due to the snow accumulation\n",
    "    Intermediate variable: \n",
    "        snow_depth[]: snowdepth of each lane at current time interval\n",
    "    '''\n",
    "    #Calculate the reduced speed based on the snow depth\n",
    "    link_delay=0\n",
    "    snow_depth_temp=0\n",
    "    snow_depth=[0]*math.floor(link_detail[link,5])\n",
    "    for t in range (math.floor(num_simulation_interval)):\n",
    "        penaltytemp=0\n",
    "        ####Calculate the snow depth\n",
    "        for snow in snow_detail:\n",
    "            if snow[2] > (t+0.5)*simulation_length and snow[1] < (t+0.5)*simulation_length:\n",
    "                current_snow=snow[0]\n",
    "        for i in range(math.floor(link_detail[link,5])):\n",
    "            if service_finished_time[link][i] <=(t+1)*simulation_length and service_finished_time[link][i] >=t*simulation_length:\n",
    "                snow_depth[i]=0\n",
    "            else:\n",
    "                snow_depth[i]=snow_depth[i]+current_snow*snow_interval/60\n",
    "        average_snow_depth=sum(snow_depth)/link_detail[link,5]\n",
    "        \n",
    "        ####Finish calculating the snow depth\n",
    "\n",
    "        ### Calculate the affected speed\n",
    "        origin_speed=link_speed[math.floor(t*simulation_length),link]\n",
    "        k=0\n",
    "        while average_snow_depth>sa_factors[k][0]:\n",
    "            k=k+1\n",
    "        speed_reduction=sa_factors[k][1]\n",
    "        capacity_reduction=sa_factors[k][2]\n",
    "        reduced_speed=origin_speed*speed_reduction #account for the speed reduction\n",
    "        #reduced_speed= #account for the capacity reduction use the BPR function\n",
    "        penaltytemp=link_volume[math.floor(t*simulation_length),link]*(origin_speed-reduced_speed)/origin_speed*simulation_length\n",
    "        link_delay=link_delay+penaltytemp\n",
    "    return link_delay\n",
    "    \n",
    "\n",
    "def calculate_service_finished_time ():\n",
    "    '''\n",
    "        Input: \n",
    "            Snowroutes: A list of nodes indicating the order of snow plow service\n",
    "        Output: \n",
    "            service_finished_time: The service finish time of each lane\n",
    "    '''\n",
    "    return\n",
    "# def path_scan(DepotLocation,LinkList):\n",
    "#     '''\n",
    "#     Input: \n",
    "#     DepotLocation: The node number of the depot\n",
    "#     LinkList: The list of link to be plowed by the snowplow\n",
    "#     Output: \n",
    "#     RoutePlanï¼š A list of nodes indicating the working order of snow plow\n",
    "#     '''\n",
    "#     #Select seed node \n",
    "#     seed_node=random.choice(LinkList)\n",
    "    \n",
    "#     #Calculate the objective function\n",
    "#     objvalue=snow_penalty()\n",
    "#     #For all different priority classes construct the inital routes\n",
    "#         #Set P empty set\n",
    "#         #for all links\n",
    "#             #Find the arc ij closet to depot/end node of class k-1 while optimize objective function\n",
    "#             #If found set end=j\n",
    "#         #Set P=(P,ij)\n",
    "#         #for all (end,i) \n",
    "#             #find the end,i that maximizes the number of non-serviced required arc of classs k adjacent to vei\n",
    "#             #set endtemp=i\n",
    "#         #if endtemp ==0 then \n",
    "#             #P=P+SP(end,0)\n",
    "#             #else\n",
    "#             #P=P+(end,endtemp)\n",
    "#             #end= endtemp\n",
    "#     #Then for all non-serviced arcs ij\n",
    "#         #add iji to the routes\n",
    "#     #Then for all non serviced arc ij\n",
    "#         #find the insertion place minimize bjective function \n",
    "#     return RoutePlan\n",
    "def C_link_to_link_distance(ServiceZone):\n",
    "    '''\n",
    "        input: ServiceZone: sorted list of link ID, indicating the links that will be served\n",
    "    '''\n",
    "    link_to_link=[[0 for x in range(len(ServiceZone))] for y in range(len(ServiceZone))] \n",
    "    i=0\n",
    "    link_to_link={}\n",
    "    for linko in ServiceZone:\n",
    "        link_to_link[linko]={}\n",
    "        for linkd in ServiceZone[i+1:]:\n",
    "            distance=distance_between_link(linko,linkd,link_xy)\n",
    "            link_to_link[linko].update({linkd:distance})\n",
    "        i=i+1\n",
    "    return link_to_link\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def path_scan_heuristic(DepotLocation,ServiceZone,service_start_time):\n",
    "    '''\n",
    "    Input: \n",
    "        DepotLocation: The node number of the depot\n",
    "        LinkList: The list of link to be plowed by the snowplow\n",
    "        service_start_time: The time the first snowplow leave the depot\n",
    "    Output: \n",
    "        RoutePlan: A list of links indicating the working order of snow plow\n",
    "    '''\n",
    "    global non_serviced_arcs,RoutePlan\n",
    "    ServiceZone=sorted(ServiceZone)\n",
    "    minobj=9*10**9\n",
    "    \n",
    "    non_serviced_arcs=[]\n",
    "    RoutePlan=[]\n",
    "    #Create a link to link distance matrix\n",
    "    link_to_link=C_link_to_link_distance(ServiceZone)\n",
    "        \n",
    "    #Duplicate arcs with i lanes into i links\n",
    "    for link in ServiceZone: \n",
    "        for i in range(math.floor(link_detail[link,5])):\n",
    "            non_serviced_arcs.extend([link])\n",
    "    num_served_arcs=0\n",
    "    link_delay={}\n",
    "    for link in ServiceZone:\n",
    "        link_delay.update({link:link_snow_penalty(link,service_finished_time)})\n",
    "\n",
    "    while non_serviced_arcs:\n",
    "        output_file=open('../test1.csv','a')\n",
    "        print('There are',num_served_arcs,'ars have been served')\n",
    "        print('Find the',num_served_arcs,'arc',file=output_file)\n",
    "        \n",
    "        linktemp=9000000\n",
    "        for link in non_serviced_arcs:\n",
    "            if link !=linktemp:\n",
    "                linktemp=link\n",
    "                target_lane=math.floor(float(np.argmax(service_finished_time[link])))\n",
    "                #Find the link that minimize the objective function\n",
    "                \n",
    "                    \n",
    "                if not RoutePlan: #if the route plan is empty, then identify the first link\n",
    "                    sv_time_temp=service_finished_time[link][target_lane]\n",
    "                    #Calculate the travel time from current node to the target link\n",
    "                    #print(0,link,minobj,link_delay[link],sum(link_delay.values()) *VOT,service_finished_time[link])\n",
    "                    if link_detail[link,0] != DepotLocation:\n",
    "                        distance_temp=math.sqrt((link_xy[link][0]-nodexy[DepotLocation][0])**2+(link_xy[link][1]-nodexy[DepotLocation][1])**2)*1.3\n",
    "                        service_finished_time[link][target_lane]=service_start_time+distance_temp/d_speed+link_detail[link,4]/s_speed/5280\n",
    "                    else:\n",
    "                        distance_temp=0\n",
    "                        service_finished_time[link][target_lane]=service_start_time+link_detail[link,4]/s_speed/5280\n",
    "                    #print(0.1,link,minobj,link_delay[link],sum(link_delay.values()) *VOT,service_finished_time[link])\n",
    "                    link_delay_temp=link_delay[link]    \n",
    "                    link_delay[link]=link_snow_penalty(link,service_finished_time)\n",
    "                    objtemp=sum(link_delay.values()) *VOT+distance_temp/d_speed*OpCost \n",
    "                    if objtemp<minobj:\n",
    "                        minobj=objtemp\n",
    "                        LinkCandidate=link\n",
    "                        can_serviced_time=service_finished_time[link][target_lane]\n",
    "                        can_delay_time=link_snow_penalty(link,service_finished_time)\n",
    "                        print(1,link,minobj,objtemp,can_delay_time,link_delay[link],sum(link_delay.values()) *VOT,service_finished_time[link],file=output_file)\n",
    "                    service_finished_time[link][target_lane]=sv_time_temp\n",
    "                    link_delay[link]=link_delay_temp\n",
    "                    if objtemp==minobj:\n",
    "                        print(2,link,minobj,objtemp,can_delay_time,link_delay[link],sum(link_delay.values()) *VOT,service_finished_time[link],file=output_file)\n",
    "                elif link_detail[link,1]!= link_detail[RoutePlan[-1],0] and link != RoutePlan[-1]:\n",
    "                    sv_time_temp=service_finished_time[link][target_lane]\n",
    "                    #Calculate the travel time from current node to the target link\n",
    "                    if link_detail[link,0] != link_detail[RoutePlan[-1],1]:\n",
    "                        distance_temp=link_to_link[min(RoutePlan[-1],link)][max(RoutePlan[-1],link)]*1.3\n",
    "                        service_finished_time[link][target_lane]=service_start_time+distance_temp/d_speed+link_detail[link,4]/s_speed/5280\n",
    "                    else:\n",
    "                        distance_temp=0\n",
    "                        service_finished_time[link][target_lane]=service_start_time+link_detail[link,4]/s_speed/5280\n",
    "                    #print(3,link,minobj,sum(link_delay.values()) *VOT,distance_temp/d_speed*OpCost,link_delay[link])\n",
    "                    link_delay_temp=link_delay[link]    \n",
    "                    link_delay[link]=link_snow_penalty(link,service_finished_time)\n",
    "                    #print(3.1,link,minobj,sum(link_delay.values()) *VOT,distance_temp/d_speed*OpCost,link_delay[link])\n",
    "                    objtemp=sum(link_delay.values()) *VOT+distance_temp/d_speed*OpCost\n",
    "\n",
    "    #                 pdb.set_trace()\n",
    "                    if objtemp<minobj:\n",
    "                        minobj=objtemp\n",
    "                        LinkCandidate=link\n",
    "                        can_serviced_time=service_finished_time[link][target_lane]\n",
    "                        print(3,link,minobj,objtemp,can_delay_time,link_delay[link],sum(link_delay.values()) *VOT,service_finished_time[link],file=output_file)\n",
    "                    service_finished_time[link][target_lane]=sv_time_temp\n",
    "                    link_delay[link]=link_delay_temp\n",
    "                    if objtemp==minobj:\n",
    "                        print(4,link,minobj,objtemp,can_delay_time,link_delay[link],sum(link_delay.values()) *VOT,service_finished_time[link],file=output_file)\n",
    "        #Updated the route plan, service finished time. Remove selected link from non serviced arcs list\n",
    "        \n",
    "        print(5,LinkCandidate,minobj,can_delay_time,link_delay[LinkCandidate],sum(link_delay.values()) *VOT,service_finished_time[LinkCandidate],file=output_file)\n",
    "        RoutePlan.extend([LinkCandidate])\n",
    "        service_finished_time[LinkCandidate][target_lane]=can_serviced_time\n",
    "#         if LinkCandidate==139:\n",
    "#             pdb.set_trace()\n",
    "#         if 139 not in non_serviced_arcs: \n",
    "#             print('stope1')\n",
    "#             pdb.set_trace()\n",
    "        if LinkCandidate not in non_serviced_arcs: \n",
    "            print('The current link candidate is not a non serviced required arc')\n",
    "            pdb.set_trace()\n",
    "        non_serviced_arcs.remove(LinkCandidate)\n",
    "#         if 139 not in non_serviced_arcs: \n",
    "#             print('stope1')\n",
    "#             pdb.set_trace()\n",
    "        service_start_time=can_serviced_time\n",
    "        link_delay[LinkCandidate]=link_snow_penalty(LinkCandidate,service_finished_time)\n",
    "        num_served_arcs=num_served_arcs+1\n",
    "        print('Link',LinkCandidate,'is added to the route.','And the service time is',can_serviced_time)\n",
    "        print(6,str(datetime.now()),LinkCandidate,minobj,can_delay_time,link_delay[LinkCandidate],sum(link_delay.values()) *VOT,service_finished_time[LinkCandidate],file=output_file)\n",
    "        #pdb.set_trace()\n",
    "        output_file.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service_finished_time=[]\n",
    "i=0\n",
    "for link in link_detail:\n",
    "    service_finished_time.append([])\n",
    "    service_finished_time[i].extend([planning_horizon]*math.floor(link[0,5]))\n",
    "    i=i+1\n",
    "\n",
    "from datetime import datetime\n",
    "path_scan_heuristic(13339,sub_service_zone[0],480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=[2,3,4,6]\n",
    "if 5 not in a:\n",
    "    print('yes')\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_serviced_arcs.remove(139)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_serviced_arcs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
